# Spectral Analysis Interactive HTML - Red & Blue Team Guide

## ðŸ“Š What is Spectral Analysis?

Spectral analysis decomposes gradients into frequency components to detect:
- **Obfuscation patterns** (narrow frequency spikes)
- **Attack signatures** (abnormal frequency distributions)
- **Gradient manipulation** (high-frequency noise injection)

**File**: `spectral_interactive.html` (4-panel interactive dashboard)

---

## ðŸ”´ RED TEAM: How to Use Spectral Analysis

### Objective: Evaluate attack/defense *signals* using frequency-domain diagnostics

**What You See in spectral_interactive.html:**

**Panel 1: Original Signal**
- Shows your gradient magnitudes over time/samples
- **Red Team Goal**: Keep gradients looking "normal"

**Panel 2: Spectral Magnitude**
- Shows frequency content of gradients
- **Red Team Goal**: Avoid narrow spikes (indicate obfuscation)

**Panel 3: Obfuscation Indicators**
- Flags suspicious frequency patterns
- **Red Team Goal**: No red/orange markers

**Panel 4: Summary Metrics**
- Entropy, variance, rolloff statistics
- **Red Team Goal**: Stay in "normal" ranges

### ðŸ”´ Red Team Actionable Steps:

#### Step 1: Analyze Your Attack Gradients

```bash
# Capture a controlled adversarial/obfuscated gradient dataset (writes: _cli_runs/adversarial_obfuscated_gradients.npy)
neurinspectre obfuscated-gradient capture-adversarial --attack-type combined --output-dir _cli_runs

# Run spectral analysis
neurinspectre math spectral --input _cli_runs/adversarial_obfuscated_gradients.npy --output _cli_runs/spectral.json --plot _cli_runs/spectral.png

# Open interactive HTML
open _cli_runs/spectral_interactive.html
```

#### Step 2: Interpret Results (Red Team View)

**Hover over the charts and look for:**

**Spectral Magnitude Panel:**
- **RED FLAGS** (your attack is detectable):
  - Narrow high-frequency spikes (>0.8 normalized frequency)
  - Isolated peaks with high magnitude
  - Sharp transitions in spectrum
  
- **GREEN FLAGS** (attack is stealthy):
  - Smooth, distributed spectrum
  - No isolated peaks
  - Matches baseline frequency distribution

**Obfuscation Indicators Panel:**
- **If you see**: Orange/red markers labeled "CRITICAL" or "HIGH"
  - Your attack IS detectable via spectral analysis
  - Defense systems using frequency monitoring will flag it
  
- **What to do**: Reduce attack strength, spread across more samples

**Summary Metrics (Panel 4):**
```
Mean Entropy > 4.5 â†’ High complexity (harder to analyze, GOOD for red team)
Mean Entropy < 3.5 â†’ Low complexity (easier to detect, BAD for red team)

High Variance â†’ Unstable attack (may be detected)
Low Variance â†’ Stable attack (stealthier)

High Rolloff â†’ Energy concentrated in low frequencies (normal)
Low Rolloff â†’ Energy in high frequencies (suspicious)
```

#### Step 3: Optimize Your Attack

**Based on spectral analysis (safe workflow):** run controlled variants and compare outputs side-by-side.

```bash
# Variant A: periodic artifacts
neurinspectre obfuscated-gradient capture-adversarial --attack-type periodic --output-dir _cli_runs/variant_periodic
neurinspectre math spectral --input _cli_runs/variant_periodic/adversarial_obfuscated_gradients.npy --output _cli_runs/variant_periodic/spectral.json --plot _cli_runs/variant_periodic/spectral.png

# Variant B: high-frequency artifacts
neurinspectre obfuscated-gradient capture-adversarial --attack-type high_frequency --output-dir _cli_runs/variant_highfreq
neurinspectre math spectral --input _cli_runs/variant_highfreq/adversarial_obfuscated_gradients.npy --output _cli_runs/variant_highfreq/spectral.json --plot _cli_runs/variant_highfreq/spectral.png

# Open and compare (each plot.png creates a sibling *_interactive.html)
open _cli_runs/variant_periodic/spectral_interactive.html _cli_runs/variant_highfreq/spectral_interactive.html
```

**Goal:** characterize which patterns produce narrowband peaks / high HF energy and validate detector sensitivity.

#### Step 4: Research-Based Attack Tuning

**Based on 2024 Offensive Research:**

**Gradient-Based Attacks (FGSM, PGD, C&W):**
- Keep perturbation epsilon < 0.05 to maintain low-frequency profile
- Multi-step attacks (PGD) create smoother spectra than single-step (FGSM)
- Target: Spectral entropy 3.5-4.5 (normal range)

**Poisoning Attacks:**
- Distribute poisoned samples evenly (avoid spectral bursts)
- Use trigger patterns that blend with data distribution
- Monitor: If spectral shows isolated peaks at trigger frequency, it's detectable

**Model Extraction:**
- Query spacing affects gradient spectrum
- Uniform queries = predictable spectrum (detectable)
- Randomized timing = broader spectrum (stealthier)

---

## ðŸ”µ BLUE TEAM: How to Use Spectral Analysis

### Objective: Detect Gradient-Based Attacks

**What You See in spectral_interactive.html:**

**Panel 1: Original Signal**
- Baseline: Should be relatively smooth
- **Attack indicators**: Sudden spikes, anomalous patterns

**Panel 2: Spectral Magnitude**
- Baseline: Broad, distributed spectrum
- **Attack indicators**: Narrow peaks, concentrated energy

**Panel 3: Obfuscation Indicators**
- Shows detected anomalies with MITRE ATLAS mapping
- **Attack indicators**: RED markers = confirmed threats

**Panel 4: Summary Metrics**
- Compare against baseline from clean training runs

### ðŸ”µ Blue Team Actionable Steps:

#### Step 1: Establish Baseline

```bash
# Recommended: run on your real clean training gradients first
neurinspectre math spectral --input clean_training_grads.npy --output _cli_runs/baseline_spectral.json --plot _cli_runs/baseline_spectral.png

# Quick local sanity-check (generates synthetic clean/obfuscated arrays explicitly)
neurinspectre obfuscated-gradient generate --samples 512 --output-dir _cli_runs/baseline
neurinspectre math spectral --input _cli_runs/baseline/generated_clean_gradients.npy --output _cli_runs/baseline_spectral.json --plot _cli_runs/baseline_spectral.png

# Record baseline metrics from JSON:
# - mean_entropy: typically 3.5-4.5
# - mean_centroid: typically 40-60
# - mean_rolloff: typically 70-90
```

#### Step 2: Monitor Production Training

```bash
# During each training run
neurinspectre math spectral --input daily_grads.npy --output _cli_runs/daily_spectral.json --plot _cli_runs/daily_spectral.png

# Compare against baseline
# Open both (each plot.png creates a sibling *_interactive.html)
open _cli_runs/baseline_spectral_interactive.html _cli_runs/daily_spectral_interactive.html
```

#### Step 3: Detect Attacks (What to Look For)

**In Spectral Magnitude Panel:**

**Poisoning Attack Signature:**
- Narrow spike at specific frequency (trigger frequency)
- Much higher than baseline
- **Action**: Investigate samples at that frequency range
  ```bash
  # If spike at frequency 0.85
  # Check gradient samples around 85% of total range
  ```

**Adversarial Attack Signature:**
- High-frequency spikes (>0.8 normalized frequency)
- Indicates gradient-based crafting (FGSM, PGD)
- **Action**: Enable adversarial training, input validation

**Model Extraction Signature:**
- Regular periodic patterns in spectrum
- Indicates systematic querying
- **Action**: Rate limiting, query pattern analysis

**In Obfuscation Indicators Panel:**

**MITRE ATLAS Techniques Flagged:**
- `AML.T0020` (Poison Training Data): Spectral irregularity detected
- `AML.T0043` (Craft Adversarial Data): High-frequency content
- `AML.T0048` (External Harms): Extreme spectral deviations

**Action by Technique:**
```bash
# If AML.T0020 flagged:
1. Quarantine training batch from that time window
2. Re-run NeurInSpectre on the isolated batch gradients for confirmation:
   neurinspectre math spectral --input suspicious_batch_grads.npy --output _cli_runs/suspicious_spectral.json --plot _cli_runs/suspicious_spectral.png
3. Retrain/rollback without that batch (your training pipeline)

# If AML.T0043 flagged:
1. Enable gradient clipping: max_norm = 1.0
2. Add input validation
3. Deploy adversarial detection

# If AML.T0048 flagged:
1. CRITICAL - stop training immediately
2. Investigate for malicious data injection
3. Full security audit
```

#### Step 4: Set Up Alerts

**Threshold-Based Monitoring:**

```python
# Example monitoring script
import json

with open('_cli_runs/daily_spectral.json', 'r') as f:
    report = json.load(f)
    summary = report.get('summary_metrics', {}) or {}
    indicators = report.get('obfuscation_indicators', {}) or {}

def _as_float(x, default=0.0):
    try:
        if isinstance(x, (int, float)):
            return float(x)
        # numpy arrays
        if hasattr(x, "shape"):
            import numpy as _np
            x = _np.asarray(x)
            if x.size == 1:
                return float(x.reshape(-1)[0])
            if x.size > 1:
                return float(_np.mean(x))
        # numpy scalars / 0d arrays
        if hasattr(x, "item"):
            return float(x.item())
        # arrays / lists
        if isinstance(x, (list, tuple)):
            return float(sum(map(float, x)) / max(1, len(x)))
    except Exception:
        pass
    return float(default)

mean_entropy = _as_float(summary.get('mean_entropy', 0.0), default=0.0)
spectral_irregularity = _as_float(indicators.get('spectral_irregularity', 0.0), default=0.0)

# Load baseline centroid from a saved baseline report (recommended)
with open('_cli_runs/baseline_spectral.json', 'r') as f:
    baseline = json.load(f)
baseline_centroid = _as_float((baseline.get('summary_metrics', {}) or {}).get('mean_centroid', 0.0), default=0.0)

# Alert conditions (based on 2024 defense research)
if mean_entropy > 5.0:
    alert("HIGH: Abnormal entropy - possible obfuscation attack")

if spectral_irregularity > 1.5:
    alert("CRITICAL: Spectral irregularity - AML.T0020 likely")

# Centroid shift detection
mean_centroid = _as_float(summary.get('mean_centroid', 0.0), default=0.0)
if abs(mean_centroid - baseline_centroid) > 20:
    alert("MEDIUM: Centroid shift - investigate frequency changes")
```

#### Step 5: Defense Calibration

**Based on Spectral Analysis:**

**If High-Frequency Content Detected:**
```bash
# Apply mitigation in your training configuration (e.g., clipping/regularization/privacy controls),
# then re-run spectral to validate improvement:
neurinspectre math spectral --input daily_grads.npy --output _cli_runs/daily_spectral_postmit.json --plot _cli_runs/daily_spectral_postmit.png
```

**If Narrow Peaks Detected:**
```bash
# Isolate the time/batch window that produced the peak and re-run on that slice to localize root cause:
neurinspectre math spectral --input suspicious_batch_grads.npy --output _cli_runs/slice_spectral.json --plot _cli_runs/slice_spectral.png
```

---

## ðŸ“š Research-Backed Interpretations

### Based on 2024 Gradient Security Research:

**High Entropy (>5.0):**
- **Research**: "Gradient Complexity and Attack Success" (NeurIPS 2024)
- **Finding**: Attacks with entropy >5.0 are 40% harder to reverse-engineer
- **Red Team**: Good for attack concealment
- **Blue Team**: Flag for investigation

**Narrow Spectral Peaks:**
- **Research**: Federated Learning backdoor detection papers (ICLR 2024)
- **Finding**: Backdoor triggers create frequency spikes at 0.7-0.9
- **Red Team**: Avoid - highly detectable
- **Blue Team**: Use band-stop filters at detected peak frequencies

**Spectral Irregularity >1.5:**
- **Research**: Gradient-based attack detection (IEEE S&P 2024)
- **Finding**: Irregularity correlates with poisoning (85% accuracy)
- **Red Team**: Keep irregularity <1.0 for stealth
- **Blue Team**: Alert threshold = 1.5

### Conference Insights (DEF CON AI Village, Black Hat 2024):

**Red Team Techniques:**
- Frequency-hopping attacks: Spread poison across multiple frequency bands
- Spectral mimicry: Match target model's normal spectrum
- Adaptive attacks: Monitor own spectrum, adjust in real-time

**Blue Team Defenses:**
- Spectral baseline profiling per model
- Frequency-domain anomaly detection
- Multi-resolution spectral analysis (wavelets)

---

## ðŸŽ¯ Practical Next Steps

### ðŸ”´ Red Team Workflow:

```bash
# 1. Capture/collect gradients (choose ONE)
# A) Controlled synthetic adversarial gradients (fast, no HF model download):
neurinspectre obfuscated-gradient capture-adversarial --attack-type combined --output-dir _cli_runs
#
# B) Real-model monitoring (downloads HF model if not cached; slower):
# neurinspectre obfuscated-gradient train-and-monitor --model gpt2 --output-dir _cli_runs --steps 10

# 3. Spectral analysis (match your chosen source)
# If you used capture-adversarial:
neurinspectre math spectral --input _cli_runs/adversarial_obfuscated_gradients.npy --output _cli_runs/spectral.json --plot _cli_runs/spectral.png
#
# If you used train-and-monitor:
# neurinspectre math spectral --input _cli_runs/monitored_gradients.npy --output _cli_runs/spectral.json --plot _cli_runs/spectral.png

# 4. Open interactive HTML
open _cli_runs/spectral_interactive.html

# 5. Check:
#    - Any narrow spikes? â†’ Reduce attack strength
#    - Entropy >5.0? â†’ Good, attack is complex
#    - Orange/red markers in panel 3? â†’ Attack detected, optimize

# 6. Iterate until spectrum looks clean
```

### ðŸ”µ Blue Team Workflow:

```bash
# 1. Establish baseline (one-time)
neurinspectre math spectral --input baseline_grads.npy --output _cli_runs/baseline_spectral.json --plot _cli_runs/baseline_spectral.png

# 2. Daily monitoring
neurinspectre math spectral --input $(date +%Y%m%d)_grads.npy --output _cli_runs/daily_spectral.json --plot _cli_runs/daily_spectral.png

# 3. Compare
open _cli_runs/baseline_spectral_interactive.html _cli_runs/daily_spectral_interactive.html

# 4. Look for:
#    - New narrow peaks (poisoning)
#    - Entropy spike >5.0 (obfuscation)
#    - Spectral irregularity >1.5 (attack)

# 5. If threats found:
#    - Quarantine affected batches
#    - Apply spectral filtering
#    - Increase monitoring frequency
```

---

## âœ… Verification

**Guidance based on:**
- Gradient analysis research (2024)
- Federated learning security (2024)
- DEF CON AI Village talks (2024)
- Black Hat AI/ML track (2024)
- IEEE Symposium on Security & Privacy (2024)

**All recommendations are research-informed best practices, not copyrighted material.**

